// // Dependencies:
// // npm install fluent-ffmpeg fs-extra node-wav webrtcvad @google-cloud/text-to-speech

// const fs = require("fs-extra");
// const path = require("path");
// const ffmpeg = require("fluent-ffmpeg");
// const wav = require("node-wav");
// const Vad = require("webrtcvad").default;
// const textToSpeech = require("@google-cloud/text-to-speech");
// const fetch = require('node-fetch');
// const FormData = require('form-data');

// const client = new textToSpeech.TextToSpeechClient();

// const inputVideo = "input.mp4";
// const tempDir = "./temp";
// const audioWav = path.join(tempDir, "audio.wav");
// const outputDir = "./output";
// const outputAudio = path.join(outputDir, "final_audio.wav");
// const transcriptFile = "./transcript.json";

// async function extractAudio() {
//   await fs.ensureDir(tempDir);
//   return new Promise((resolve, reject) => {
//     ffmpeg(inputVideo)
//       .noVideo()
//       .audioCodec("pcm_s16le")
//       .audioChannels(1)
//       .audioFrequency(16000)
//       .save(audioWav)
//       .on("end", resolve)
//       .on("error", reject);
//   });
// }

// function detectSpeechSegments(buffer, sampleRate = 16000, aggressiveness = 2) {
//   const vad = new Vad(sampleRate, aggressiveness);
//   const frameDuration = 30; // ms
//   const frameLength = (sampleRate * frameDuration * 2) / 1000;
//   const samples = new Int16Array(buffer.buffer);
//   const segments = [];
//   let inSpeech = false;
//   let speechStart = 0;

//   for (let i = 0; i < samples.length; i += frameLength) {
//     const frame = samples.slice(i, i + frameLength);
//     if (frame.length < frameLength) break;
//     const isSpeech = vad.process(frame, sampleRate);
//     const timestamp = i / sampleRate;

//     if (isSpeech && !inSpeech) {
//       inSpeech = true;
//       speechStart = timestamp;
//     } else if (!isSpeech && inSpeech) {
//       inSpeech = false;
//       segments.push({ start: speechStart, end: timestamp });
//     }
//   }
//   if (inSpeech) {
//     segments.push({ start: speechStart, end: samples.length / sampleRate });
//   }
//   return segments;
// }

// async function synthesizeSpeech(text, speed, index) {
//   const request = {
//     input: { text },
//     voice: { languageCode: "gu-IN" /* name: "gu-IN-chirp3-HD-Archid" */ },
//     audioConfig: { audioEncoding: "MP3", speakingRate: speed },
//   };
//   const [response] = await client.synthesizeSpeech(request);
//   const filePath = path.join(outputDir, `segment_${index}.mp3`);
//   await fs.writeFile(filePath, response.audioContent, "binary");
//   return filePath;
// }

// async function generateSilence(duration, index) {
//   return new Promise((resolve, reject) => {
//     const filePath = path.join(outputDir, `silence_${index}.mp3`);
//     ffmpeg()
//       .input("anullsrc")
//       .inputFormat("lavfi")
//       .duration(duration)
//       .output(filePath)
//       .on("end", () => resolve(filePath))
//       .on("error", reject)
//       .run();
//   });
// }

// async function processTranscriptAndSegments(segments) {
//   const transcript = await fs.readJson(transcriptFile); // [{ text, speed }]
//   const audioParts = [];
//   let tIndex = 0;

//   for (let i = 0; i < segments.length; i++) {
//     const seg = segments[i];
//     const segDuration = seg.end - seg.start;
//     if (tIndex >= transcript.length) break;
//     const entry = transcript[tIndex];

//     if (entry.text.trim() === "") {
//       const silenceFile = await generateSilence(segDuration, `silence_${i}`);
//       audioParts.push(silenceFile);
//       continue;
//     }

//     const voiceFile = await synthesizeSpeech(
//       entry.text,
//       entry.speed,
//       `voice_${i}`
//     );
//     audioParts.push(voiceFile);
//     tIndex++;
//   }
//   return audioParts;
// }

// async function mergeAudioParts(parts) {
//   return new Promise((resolve, reject) => {
//     const ff = ffmpeg();
//     parts.forEach((p) => ff.input(p));
//     ff.mergeToFile(outputAudio, tempDir)
//       .on("end", () => {
//         console.log("✅ Final audio saved:", outputAudio);
//         resolve();
//       })
//       .on("error", reject);
//   });
// }

// async function analyzeAudioAndSplit() {
//   const audioData = fs.readFileSync(audioWav);
//   const result = wav.decode(audioData);
//   const speechSegments = detectSpeechSegments(
//     result.channelData[0],
//     result.sampleRate
//   );
//   return speechSegments;
// }

// async function main() {
//   await fs.ensureDir(outputDir);
//   await extractAudio();
//   const segments = await analyzeAudioAndSplit();
//   const audioParts = await processTranscriptAndSegments(segments);
//   await mergeAudioParts(audioParts);
// }

// main().catch(console.error);
// Dependencies:
// npm install fluent-ffmpeg fs-extra node-wav webrtcvad @google-cloud/text-to-speech node-fetch form-data

// const fs = require("fs-extra");
// const path = require("path");
// const ffmpeg = require("fluent-ffmpeg");
// const wav = require("node-wav");
// const Vad = require("webrtcvad");
// const textToSpeech = require("@google-cloud/text-to-speech");
// const fetch = require("node-fetch");
// const FormData = require("form-data");

// const client = new textToSpeech.TextToSpeechClient();

// const inputVideo = "input.mp4";
// const tempDir = "./temp";
// const audioWav = path.join(tempDir, "audio.wav");
// const outputDir = "./output";
// const outputAudio = path.join(outputDir, "final_audio.wav");
// const gujaratiTranslations = require("./gujarati.json"); // array of Gujarati sentences

// async function extractAudio() {
//   await fs.ensureDir(tempDir);
//   return new Promise((resolve, reject) => {
//     ffmpeg(inputVideo)
//       .noVideo()
//       .audioCodec("pcm_s16le")
//       .audioChannels(1)
//       .audioFrequency(16000)
//       .save(audioWav)
//       .on("end", resolve)
//       .on("error", reject);
//   });
// }

// async function transcribeWithWhisper(audioPath) {
//   const form = new FormData();
//   form.append("file", fs.createReadStream(audioPath));
//   form.append("model", "whisper-1");
//   form.append("response_format", "verbose_json");

//   const response = await fetch(
//     "https://api.openai.com/v1/audio/transcriptions",
//     {
//       method: "POST",
//       headers: {
//         Authorization: `Bearer ${process.env.OPENAI_API_KEY}`,
//       },
//       body: form,
//     }
//   );

//   if (!response.ok) {
//     throw new Error(`Whisper API error: ${response.statusText}`);
//   }

//   const result = await response.json();
//   return result.segments.map((s, i) => ({
//     id: i,
//     text: s.text.trim(),
//     start: s.start,
//     end: s.end,
//   }));
// }

// async function synthesizeSpeech(text, speed, index) {
//   const request = {
//     input: { text },
//     voice: { languageCode: "gu-IN", name: "gu-IN-chirp3-HD-Archid" },
//     audioConfig: { audioEncoding: "MP3", speakingRate: speed },
//   };
//   const [response] = await client.synthesizeSpeech(request);
//   const filePath = path.join(outputDir, `segment_${index}.mp3`);
//   await fs.writeFile(filePath, response.audioContent, "binary");
//   return filePath;
// }

// async function generateSilence(duration, index) {
//   return new Promise((resolve, reject) => {
//     const filePath = path.join(outputDir, `silence_${index}.mp3`);
//     ffmpeg()
//       .input("anullsrc")
//       .inputFormat("lavfi")
//       .duration(duration)
//       .output(filePath)
//       .on("end", () => resolve(filePath))
//       .on("error", reject)
//       .run();
//   });
// }

// async function processTranscriptWithWhisperAlignment(segments, translations) {
//   const audioParts = [];
//   for (let i = 0; i < segments.length; i++) {
//     const seg = segments[i];
//     const duration = seg.end - seg.start;
//     const sentence = translations[i]?.text || "";
//     const speed = translations[i]?.speed || 1.0;

//     if (!sentence.trim()) {
//       const silenceFile = await generateSilence(duration, `silence_${i}`);
//       audioParts.push(silenceFile);
//     } else {
//       const voiceFile = await synthesizeSpeech(sentence, speed, `voice_${i}`);
//       audioParts.push(voiceFile);
//     }
//   }
//   return audioParts;
// }

// async function mergeAudioParts(parts) {
//   return new Promise((resolve, reject) => {
//     const ff = ffmpeg();
//     parts.forEach((p) => ff.input(p));
//     ff.mergeToFile(outputAudio, tempDir)
//       .on("end", () => {
//         console.log("✅ Final audio saved:", outputAudio);
//         resolve();
//       })
//       .on("error", reject);
//   });
// }

// async function main() {
//   await fs.ensureDir(outputDir);
//   await extractAudio();
//   const segments = await transcribeWithWhisper(audioWav);
//   const audioParts = await processTranscriptWithWhisperAlignment(
//     segments,
//     gujaratiTranslations
//   );
//   await mergeAudioParts(audioParts);
// }

// main().catch(console.error);
